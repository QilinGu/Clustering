(True/False) Skipping data points (i.e., skipping rows of the data) that have missing features only works when the learning algorithm we are using is decision tree learning.
False

What are potential downsides of skipping features with missing values (i.e., skipping columns of the data) to handle missing data?
So many features are skipped that accuracy can degrade
You will have fewer data points (i.e., rows) in the dataset (W)
a

(True/False) Itâ€™s always better to remove missing data points (i.e., rows) as opposed to removing missing features (i.e., columns).
False

Consider a dataset with N training points. After imputing missing values, the number of data points in the data set is
N

Consider a dataset with D features. After imputing missing values, the number of features in the data set is
D

Which of the following are always true when imputing missing data? Select all that apply.
a,b

Consider data that has binary features (i.e. the feature values are 0 or 1) with some feature values of some data points missing. When learning the best feature split at a node, how would we best modify the decision tree learning algorithm to handle data points with missing values for a feature?
We choose to assign missing values to the branch of the tree (either the one with feature value equal to 0 or with feature value equal to 1) that minimizes classification error.